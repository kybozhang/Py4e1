{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Capstone Projects\n",
    "\n",
    "## Table of Contents\n",
    "1. [Search engine](#search) : [\\[Spider\\]](#Spider) - [\\[Sprank\\]](#Sprank) - [\\[Spdump\\]](#Spdump) - [\\[Spjson\\]](#Spjson)\n",
    "2. [Email data modeling](#email): [\\[Gmane\\]](#gmane) - [\\[Gmodel\\]](#gmodel)\n",
    "3. [Email data visualization](#emviz): [\\[Gbasic\\]](#gbasic) - [\\[Gline\\]](#gline) - [\\[Gword\\]](#gword)  \n",
    ". [](#)\n",
    ". [](#)\n",
    ". [](#)\n",
    ". [](#)  \n",
    ". [](#): [\\[\\]](#) - [\\[\\]](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Search engines <a name='earch'></a>\n",
    ">Search engine architecture: web crawling -> index building -> searching\n",
    "- Web crawler: retrieve a page -> look through page for links -> add all links to the 'to be retrieved' list -> loop...\n",
    "- Web crawling policies:\n",
    "    - selection: which page to retrieve\n",
    "    - re-visit:: when to check for updates\n",
    "    - politeness: avoid overloading the site\n",
    "    - parallelization: how to corrdinate distributed web crawlers\n",
    "- 'robots.txt' : a way for website to communicate with web crawlers (voluntary), sometimes making a spider trap\n",
    "\n",
    "### Spider\n",
    "Crawl over the web (can focus on a specific domain name) and retrieve the inner link network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\n",
      "['http://www.dr-chuck.com', '10', '11']\n",
      "How many pages:20\n",
      "7 11 Unable to retrieve or parse page\n",
      "No unretrieved HTML pages found\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('./pagerank/spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages\n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links\n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check to see if we are already in progress...\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\n",
    "else :\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n",
    "        conn.commit()\n",
    "\n",
    "# Get the current webs\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "many = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        sval = input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        # print row\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "    \n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urlopen(url, context=ctx)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error on page: \",document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type() :\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('('+str(len(html))+')', end=' ')\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        # print href\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "\t\t# Check if the URL is in any of the webs\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "        # print fromid, toid\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n",
    "\n",
    "\n",
    "    print(count)\n",
    "\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sprank\n",
    ">Page rank: loop `old rank / no. of outbound id` (maybe total outbound rank values?) corrected by evaporation value `(old total - new total)/no. of pages` to avoid looping trap. Converged over time and can be dynamically computed for growing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations:1\n",
      "1 0.0\n",
      "[(1, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('./pagerank/spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Find the ids that send out page rank - we only are interested\n",
    "# in pages in the SCC that have in and out links\n",
    "cur.execute('''SELECT DISTINCT from_id FROM Links''')\n",
    "from_ids = list()\n",
    "for row in cur: \n",
    "    from_ids.append(row[0])\n",
    "\n",
    "# Find the ids that receive page rank \n",
    "to_ids = list()\n",
    "links = list()\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "for row in cur:\n",
    "    from_id = row[0]\n",
    "    to_id = row[1]\n",
    "    if from_id == to_id : continue\n",
    "    if from_id not in from_ids : continue\n",
    "    if to_id not in from_ids : continue\n",
    "    links.append(row)\n",
    "    if to_id not in to_ids : to_ids.append(to_id)\n",
    "\n",
    "# Get latest page ranks for strongly connected component\n",
    "prev_ranks = dict()\n",
    "for node in from_ids:\n",
    "    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n",
    "    row = cur.fetchone()\n",
    "    prev_ranks[node] = row[0]\n",
    "\n",
    "sval = input('How many iterations:')\n",
    "many = 1\n",
    "if ( len(sval) > 0 ) : many = int(sval)\n",
    "\n",
    "# Sanity check\n",
    "if len(prev_ranks) < 1 : \n",
    "    print(\"Nothing to page rank.  Check data.\")\n",
    "    quit()\n",
    "\n",
    "# Lets do Page Rank in memory so it is really fast\n",
    "for i in range(many):\n",
    "    # print prev_ranks.items()[:5]\n",
    "    next_ranks = dict();\n",
    "    total = 0.0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        total = total + old_rank\n",
    "        next_ranks[node] = 0.0\n",
    "    # print total\n",
    "\n",
    "    # Find the number of outbound links and sent the page rank down each\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        # print node, old_rank\n",
    "        give_ids = list()\n",
    "        for (from_id, to_id) in links:\n",
    "            if from_id != node : continue\n",
    "           #  print '   ',from_id,to_id\n",
    "\n",
    "            if to_id not in to_ids: continue\n",
    "            give_ids.append(to_id)\n",
    "        if ( len(give_ids) < 1 ) : continue\n",
    "        amount = old_rank / len(give_ids)\n",
    "        # print node, old_rank,amount, give_ids\n",
    "    \n",
    "        for id in give_ids:\n",
    "            next_ranks[id] = next_ranks[id] + amount\n",
    "    \n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "    evap = (total - newtot) / len(next_ranks)\n",
    "\n",
    "    # print newtot, evap\n",
    "    for node in next_ranks:\n",
    "        next_ranks[node] = next_ranks[node] + evap\n",
    "\n",
    "    newtot = 0\n",
    "    for (node, next_rank) in list(next_ranks.items()):\n",
    "        newtot = newtot + next_rank\n",
    "\n",
    "    # Compute the per-page average change from old rank to new rank\n",
    "    # As indication of convergence of the algorithm\n",
    "    totdiff = 0\n",
    "    for (node, old_rank) in list(prev_ranks.items()):\n",
    "        new_rank = next_ranks[node]\n",
    "        diff = abs(old_rank-new_rank)\n",
    "        totdiff = totdiff + diff\n",
    "\n",
    "    avediff = totdiff / len(prev_ranks)\n",
    "    print(i+1, avediff)\n",
    "\n",
    "    # rotate\n",
    "    prev_ranks = next_ranks\n",
    "\n",
    "# Put the final ranks back into the database\n",
    "print(list(next_ranks.items())[:5])\n",
    "cur.execute('''UPDATE Pages SET old_rank=new_rank''')\n",
    "for (id, new_rank) in list(next_ranks.items()) :\n",
    "    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spdump\n",
    "Group the link table joined with page table on to_ids and group by the to_id (therefore counting the inbound links)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('./pagerank/spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "     FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "     WHERE html IS NOT NULL\n",
    "     GROUP BY id ORDER BY inbound DESC''')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    if count < 50 : print(row)\n",
    "    count = count + 1\n",
    "print(count, 'rows.')\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spjson\n",
    "Set the no. of page to visualize, calculate the circle radius by the rank and connection linewidth by weight(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('./pagerank/spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Creating JSON output on spider.js...\")\n",
    "howmany = int(input(\"How many nodes? \"))\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "    FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "    WHERE html IS NOT NULL AND ERROR IS NULL\n",
    "    GROUP BY id ORDER BY id,inbound''')\n",
    "\n",
    "fhand = open('./pagerank/spider.js','w')\n",
    "nodes = list()\n",
    "maxrank = None\n",
    "minrank = None\n",
    "for row in cur :\n",
    "    nodes.append(row)\n",
    "    rank = row[2]\n",
    "    if maxrank is None or maxrank < rank: maxrank = rank\n",
    "    if minrank is None or minrank > rank : minrank = rank\n",
    "    if len(nodes) > howmany : break\n",
    "\n",
    "if maxrank == minrank or maxrank is None or minrank is None:\n",
    "    print(\"Error - please run sprank.py to compute page rank\")\n",
    "    quit()\n",
    "\n",
    "fhand.write('spiderJson = {\"nodes\":[\\n')\n",
    "count = 0\n",
    "map = dict()\n",
    "ranks = dict()\n",
    "for row in nodes :\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    # print row\n",
    "    rank = row[2]\n",
    "    rank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{'+'\"weight\":'+str(row[0])+',\"rank\":'+str(rank)+',')\n",
    "    fhand.write(' \"id\":'+str(row[3])+', \"url\":\"'+row[4]+'\"}')\n",
    "    map[row[3]] = count\n",
    "    ranks[row[3]] = rank\n",
    "    count = count + 1\n",
    "fhand.write('],\\n')\n",
    "\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "fhand.write('\"links\":[\\n')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    # print row\n",
    "    if row[0] not in map or row[1] not in map : continue\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    rank = ranks[row[0]]\n",
    "    srank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{\"source\":'+str(map[row[0]])+',\"target\":'+str(map[row[1]])+',\"value\":3}')\n",
    "    count = count + 1\n",
    "fhand.write(']};')\n",
    "fhand.close()\n",
    "cur.close()\n",
    "\n",
    "print(\"Open force.html in a browser to view the visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results demoed in the [HTML file](./pagerank/force.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Email data modeling <a name='email'></a>\n",
    "(Most of the scripts in part 2 and [3](#emviz) are NOT executable.)\n",
    ">Analyzing an EMAIL Archive from gmane and vizualizing the data\n",
    "using the D3 JavaScript library\n",
    "\n",
    "This is a set of tools that allow you to pull down an archive\n",
    "of a gmane repository using the instructions at:  \n",
    "http://gmane.org/export.php\n",
    "\n",
    "In order not to overwhelm the gmane.org server, a copy of the messages is stored at a faster server:  \n",
    "http://mbox.dr-chuck.net/\n",
    "\n",
    "### Gmane.py <a name='gmane'></a>\n",
    "Just crawl over the database and get the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date is OK\\n']\n",
      "['Date is OK\\n', 'Late of the day\\n', 'Date \\n', 'Date of the day\\n', 'Eate of the day\\n']\n",
      "['\\nDate of the day\\n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "test = '''Date is OK\\n\n",
    "Late of the day\\n\n",
    "Mata of the day Date \\n\n",
    "Date of the day\\n\n",
    "Eate of the day\n",
    "'''\n",
    "x = re.findall('^Date .*\\n', test) # only the start of a string, not internal line start\n",
    "y = re.findall('\\Date .*\\n', test) # \\D means any non-digit character\n",
    "z = re.findall('\\nDate .*\\n', test) # Date after an internal line change\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import ssl\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Not all systems have this so conditionally define parser\n",
    "try:\n",
    "    import dateutil.parser as parser\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def parsemaildate(md) :\n",
    "    # See if we have dateutil\n",
    "    try:\n",
    "        pdate = parser.parse(tdate)\n",
    "        test_at = pdate.isoformat()\n",
    "        return test_at\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Non-dateutil version - we try our best\n",
    "\n",
    "    pieces = md.split()\n",
    "    notz = \" \".join(pieces[:4]).strip()\n",
    "\n",
    "    # Try a bunch of format variations - strptime() is *lame*\n",
    "    dnotz = None\n",
    "    for form in [ '%d %b %Y %H:%M:%S', '%d %b %Y %H:%M:%S',\n",
    "        '%d %b %Y %H:%M', '%d %b %Y %H:%M', '%d %b %y %H:%M:%S',\n",
    "        '%d %b %y %H:%M:%S', '%d %b %y %H:%M', '%d %b %y %H:%M' ] :\n",
    "        try:\n",
    "            dnotz = datetime.strptime(notz, form)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if dnotz is None :\n",
    "        # print 'Bad Date:',md\n",
    "        return None\n",
    "\n",
    "    iso = dnotz.isoformat()\n",
    "\n",
    "    tz = \"+0000\"\n",
    "    try:\n",
    "        tz = pieces[4]\n",
    "        ival = int(tz) # Only want numeric timezone values\n",
    "        if tz == '-0000' : tz = '+0000'\n",
    "        tzh = tz[:3]\n",
    "        tzm = tz[3:]\n",
    "        tz = tzh+\":\"+tzm\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return iso+tz\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('content.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "baseurl = \"http://mbox.dr-chuck.net/sakai.devel/\"\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Messages\n",
    "    (id INTEGER UNIQUE, email TEXT, sent_at TEXT,\n",
    "     subject TEXT, headers TEXT, body TEXT)''')\n",
    "\n",
    "# Pick up where we left off\n",
    "start = None\n",
    "cur.execute('SELECT max(id) FROM Messages' )\n",
    "try:\n",
    "    row = cur.fetchone()\n",
    "    if row is None :\n",
    "        start = 0\n",
    "    else:\n",
    "        start = row[0]\n",
    "except:\n",
    "    start = 0\n",
    "\n",
    "if start is None : start = 0\n",
    "\n",
    "many = 0\n",
    "count = 0\n",
    "fail = 0\n",
    "while True:\n",
    "    if ( many < 1 ) :\n",
    "        conn.commit()\n",
    "        sval = input('How many messages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        many = int(sval)\n",
    "\n",
    "    start = start + 1 # bacause sql starts at 1 instead of 0???\n",
    "    cur.execute('SELECT id FROM Messages WHERE id=?', (start,) )\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        if row is not None : continue\n",
    "    except:\n",
    "        row = None\n",
    "\n",
    "    many = many - 1\n",
    "    url = baseurl + str(start) + '/' + str(start + 1)\n",
    "\n",
    "    text = \"None\"\n",
    "    try:\n",
    "        # Open with a timeout of 30 seconds\n",
    "        document = urllib.request.urlopen(url, None, 30, context=ctx)\n",
    "        text = document.read().decode()\n",
    "        if document.getcode() != 200 :\n",
    "            print(\"Error code=\",document.getcode(), url)\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print('')\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Unable to retrieve or parse page\",url)\n",
    "        print(\"Error\",e)\n",
    "        fail = fail + 1\n",
    "        if fail > 5 : break # tolerate up to 5 misses\n",
    "        continue\n",
    "\n",
    "    print(url,len(text))\n",
    "    count = count + 1\n",
    "    \n",
    "    # it should startwith from \n",
    "    if not text.startswith(\"From \"):\n",
    "        print(text)\n",
    "        print(\"Did not find From \")\n",
    "        fail = fail + 1\n",
    "        if fail > 5 : break\n",
    "        continue\n",
    "\n",
    "    pos = text.find(\"\\n\\n\") # indicates a blank line spacing\n",
    "    # wheere the body of email begins \n",
    "    if pos > 0 :\n",
    "        hdr = text[:pos]\n",
    "        body = text[pos+2:]\n",
    "    else:\n",
    "        print(text)\n",
    "        print(\"Could not find break between headers and body\")\n",
    "        fail = fail + 1\n",
    "        if fail > 5 : break\n",
    "        continue\n",
    "\n",
    "    email = None\n",
    "    x = re.findall('\\nFrom: .* <(\\S+@\\S+)>\\n', hdr)\n",
    "    # () is used in regex for extracting\n",
    "    if len(x) == 1 :\n",
    "        email = x[0];\n",
    "        email = email.strip().lower()\n",
    "        email = email.replace(\"<\",\"\")\n",
    "    else:\n",
    "        x = re.findall('\\nFrom: (\\S+@\\S+)\\n', hdr)\n",
    "        if len(x) == 1 :\n",
    "            email = x[0];\n",
    "            email = email.strip().lower()\n",
    "            email = email.replace(\"<\",\"\")\n",
    "\n",
    "    date = None\n",
    "    y = re.findall('\\nDate: .*, (.*)\\n', hdr)\n",
    "    if len(y) == 1 :\n",
    "        tdate = y[0]\n",
    "        tdate = tdate[:26]\n",
    "        try:\n",
    "            sent_at = parsemaildate(tdate)\n",
    "        except:\n",
    "            print(text)\n",
    "            print(\"Parse fail\",tdate)\n",
    "            fail = fail + 1\n",
    "            if fail > 5 : break\n",
    "            continue\n",
    "\n",
    "    subject = None\n",
    "    z = re.findall('\\nSubject: (.*)\\n', hdr)\n",
    "    if len(z) == 1 : subject = z[0].strip().lower();\n",
    "\n",
    "    # Reset the fail counter\n",
    "    fail = 0\n",
    "    print(\"   \",email,sent_at,subject)\n",
    "    cur.execute('''INSERT OR IGNORE INTO Messages (id, email, sent_at, subject, headers, body)\n",
    "        VALUES ( ?, ?, ?, ?, ?, ? )''', ( start, email, sent_at, subject, hdr, body))\n",
    "    if count % 50 == 0 : conn.commit()\n",
    "        # since it commit every 50 read\n",
    "    if count % 100 == 0 : time.sleep(1)\n",
    "# you need to commit in the end\n",
    "conn.commit()\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gmodel.py  <a name='gmodel'></a>\n",
    "Clean the data to reduce database size. <br>\n",
    "Use `zlib.compress()` to compress long str data (such as the full header and body of the email).\n",
    ">__Data model__ after clean up:\n",
    "- Messages: id, guid, sent_at, sender_id, subject_id, headers BLOB, body BLOB\n",
    "- Senders: id, sender\n",
    "- Subjects: id, subject TEXT\n",
    "- Replies (many-to-many): from_id, to_id\n",
    "- Mapping (manually done): old_email, new_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import re\n",
    "import zlib # a way to conpress text data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Not all systems have this\n",
    "try:\n",
    "    import dateutil.parser as parser\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dnsmapping = dict()\n",
    "mapping = dict()\n",
    "\n",
    "def fixsender(sender,allsenders=None) :\n",
    "    global dnsmapping\n",
    "    global mapping\n",
    "    if sender is None : return None\n",
    "    sender = sender.strip().lower()\n",
    "    sender = sender.replace('<','').replace('>','')\n",
    "\n",
    "    # Check if we have a hacked gmane.org from address\n",
    "    if allsenders is not None and sender.endswith('gmane.org') :\n",
    "        pieces = sender.split('-')\n",
    "        realsender = None\n",
    "        for s in allsenders:\n",
    "            if s.startswith(pieces[0]) :\n",
    "                realsender = sender\n",
    "                sender = s\n",
    "                # print(realsender, sender)\n",
    "                break\n",
    "        if realsender is None :\n",
    "            for s in mapping:\n",
    "                if s.startswith(pieces[0]) :\n",
    "                    realsender = sender\n",
    "                    sender = mapping[s]\n",
    "                    # print(realsender, sender)\n",
    "                    break\n",
    "        if realsender is None : sender = pieces[0]\n",
    "\n",
    "    mpieces = sender.split(\"@\")\n",
    "    if len(mpieces) != 2 : return sender\n",
    "    dns = mpieces[1]\n",
    "    x = dns\n",
    "    pieces = dns.split(\".\")\n",
    "    if dns.endswith(\".edu\") or dns.endswith(\".com\") or dns.endswith(\".org\") or dns.endswith(\".net\") :\n",
    "        dns = \".\".join(pieces[-2:])\n",
    "    else:\n",
    "        dns = \".\".join(pieces[-3:])\n",
    "    # if dns != x : print(x,dns)\n",
    "    # if dns != dnsmapping.get(dns,dns) : print(dns,dnsmapping.get(dns,dns))\n",
    "    dns = dnsmapping.get(dns,dns)\n",
    "    return mpieces[0] + '@' + dns\n",
    "\n",
    "def parsemaildate(md) :\n",
    "    # See if we have dateutil\n",
    "    try:\n",
    "        pdate = parser.parse(md)\n",
    "        test_at = pdate.isoformat()\n",
    "        return test_at\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Non-dateutil version - we try our best\n",
    "\n",
    "    pieces = md.split()\n",
    "    notz = \" \".join(pieces[:4]).strip()\n",
    "\n",
    "    # Try a bunch of format variations - strptime() is *lame*\n",
    "    dnotz = None\n",
    "    for form in [ '%d %b %Y %H:%M:%S', '%d %b %Y %H:%M:%S',\n",
    "        '%d %b %Y %H:%M', '%d %b %Y %H:%M', '%d %b %y %H:%M:%S',\n",
    "        '%d %b %y %H:%M:%S', '%d %b %y %H:%M', '%d %b %y %H:%M' ] :\n",
    "        try:\n",
    "            dnotz = datetime.strptime(notz, form)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if dnotz is None :\n",
    "        # print('Bad Date:',md)\n",
    "        return None\n",
    "\n",
    "    iso = dnotz.isoformat()\n",
    "\n",
    "    tz = \"+0000\"\n",
    "    try:\n",
    "        tz = pieces[4]\n",
    "        ival = int(tz) # Only want numeric timezone values\n",
    "        if tz == '-0000' : tz = '+0000'\n",
    "        tzh = tz[:3]\n",
    "        tzm = tz[3:]\n",
    "        tz = tzh+\":\"+tzm\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return iso+tz\n",
    "\n",
    "# Parse out the info...\n",
    "def parseheader(hdr, allsenders=None):\n",
    "    if hdr is None or len(hdr) < 1 : return None\n",
    "    sender = None\n",
    "    x = re.findall('\\nFrom: .* <(\\S+@\\S+)>\\n', hdr)\n",
    "    if len(x) >= 1 :\n",
    "        sender = x[0]\n",
    "    else:\n",
    "        x = re.findall('\\nFrom: (\\S+@\\S+)\\n', hdr)\n",
    "        if len(x) >= 1 :\n",
    "            sender = x[0]\n",
    "\n",
    "    # normalize the domain name of Email addresses\n",
    "    sender = fixsender(sender, allsenders)\n",
    "\n",
    "    date = None\n",
    "    y = re.findall('\\nDate: .*, (.*)\\n', hdr)\n",
    "    sent_at = None\n",
    "    if len(y) >= 1 :\n",
    "        tdate = y[0]\n",
    "        tdate = tdate[:26]\n",
    "        try:\n",
    "            sent_at = parsemaildate(tdate)\n",
    "        except Exception as e:\n",
    "            # print('Date ignored ',tdate, e)\n",
    "            return None\n",
    "\n",
    "    subject = None\n",
    "    z = re.findall('\\nSubject: (.*)\\n', hdr)\n",
    "    if len(z) >= 1 : subject = z[0].strip().lower()\n",
    "\n",
    "    guid = None\n",
    "    z = re.findall('\\nMessage-ID: (.*)\\n', hdr)\n",
    "    if len(z) >= 1 : guid = z[0].strip().lower()\n",
    "\n",
    "    if sender is None or sent_at is None or subject is None or guid is None :\n",
    "        return None\n",
    "    return (guid, sender, subject, sent_at)\n",
    "#===================================\n",
    "# Main\n",
    "#===================================\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS Messages ''')\n",
    "cur.execute('''DROP TABLE IF EXISTS Senders ''')\n",
    "cur.execute('''DROP TABLE IF EXISTS Subjects ''')\n",
    "cur.execute('''DROP TABLE IF EXISTS Replies ''')\n",
    "\n",
    "# guid is global unique id (of the email?)\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Messages\n",
    "    (id INTEGER PRIMARY KEY, guid TEXT UNIQUE, sent_at INTEGER,\n",
    "     sender_id INTEGER, subject_id INTEGER,\n",
    "     headers BLOB, body BLOB)''')\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Senders\n",
    "    (id INTEGER PRIMARY KEY, sender TEXT UNIQUE)''')\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Subjects\n",
    "    (id INTEGER PRIMARY KEY, subject TEXT UNIQUE)''')\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Replies\n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "conn_1 = sqlite3.connect('mapping.sqlite')\n",
    "cur_1 = conn_1.cursor()\n",
    "\n",
    "cur_1.execute('''SELECT old,new FROM DNSMapping''')\n",
    "for message_row in cur_1 :\n",
    "    dnsmapping[message_row[0].strip().lower()] = message_row[1].strip().lower()\n",
    "\n",
    "mapping = dict()\n",
    "cur_1.execute('''SELECT old,new FROM Mapping''')\n",
    "for message_row in cur_1 :\n",
    "    old = fixsender(message_row[0])\n",
    "    new = fixsender(message_row[1])\n",
    "    mapping[old] = fixsender(new)\n",
    "\n",
    "# Done with mapping.sqlite\n",
    "conn_1.close()\n",
    "\n",
    "# Open the main content (Read only)\n",
    "conn_1 = sqlite3.connect('file:content.sqlite?mode=ro', uri=True)\n",
    "cur_1 = conn_1.cursor()\n",
    "\n",
    "allsenders = list()\n",
    "cur_1.execute('''SELECT email FROM Messages''')\n",
    "for message_row in cur_1 :\n",
    "    sender = fixsender(message_row[0])\n",
    "    if sender is None : continue\n",
    "    if 'gmane.org' in sender : continue\n",
    "    if sender in allsenders: continue\n",
    "    allsenders.append(sender)\n",
    "\n",
    "print(\"Loaded allsenders\",len(allsenders),\"and mapping\",len(mapping),\"dns mapping\",len(dnsmapping))\n",
    "\n",
    "cur_1.execute('''SELECT headers, body, sent_at\n",
    "    FROM Messages ORDER BY sent_at''')\n",
    "\n",
    "senders = dict()\n",
    "subjects = dict()\n",
    "guids = dict()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for message_row in cur_1 :\n",
    "    hdr = message_row[0]\n",
    "    parsed = parseheader(hdr, allsenders)\n",
    "    if parsed is None: continue\n",
    "    (guid, sender, subject, sent_at) = parsed\n",
    "\n",
    "    # Apply the sender mapping\n",
    "    sender = mapping.get(sender,sender)\n",
    "\n",
    "    count = count + 1\n",
    "    if count % 250 == 1 : print(count,sent_at, sender)\n",
    "    # print(guid, sender, subject, sent_at)\n",
    "\n",
    "    if 'gmane.org' in sender:\n",
    "        print(\"Error in sender ===\", sender)\n",
    "\n",
    "    sender_id = senders.get(sender,None)\n",
    "    subject_id = subjects.get(subject,None)\n",
    "    guid_id = guids.get(guid,None)\n",
    "\n",
    "    if sender_id is None :\n",
    "        cur.execute('INSERT OR IGNORE INTO Senders (sender) VALUES ( ? )', ( sender, ) )\n",
    "        conn.commit()\n",
    "        cur.execute('SELECT id FROM Senders WHERE sender=? LIMIT 1', ( sender, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            sender_id = row[0]\n",
    "            senders[sender] = sender_id\n",
    "        except:\n",
    "            print('Could not retrieve sender id',sender)\n",
    "            break\n",
    "    if subject_id is None :\n",
    "        cur.execute('INSERT OR IGNORE INTO Subjects (subject) VALUES ( ? )', ( subject, ) )\n",
    "        conn.commit()\n",
    "        cur.execute('SELECT id FROM Subjects WHERE subject=? LIMIT 1', ( subject, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            subject_id = row[0]\n",
    "            subjects[subject] = subject_id\n",
    "        except:\n",
    "            print('Could not retrieve subject id',subject)\n",
    "            break\n",
    "    # print(sender_id, subject_id)\n",
    "    cur.execute('INSERT OR IGNORE INTO Messages (guid,sender_id,subject_id,sent_at,headers,body) VALUES ( ?,?,?,datetime(?),?,? )',\n",
    "            ( guid, sender_id, subject_id, sent_at,\n",
    "            zlib.compress(message_row[0].encode()), zlib.compress(message_row[1].encode())) )\n",
    "            # compress the header and the body str\n",
    "    conn.commit()\n",
    "    cur.execute('SELECT id FROM Messages WHERE guid=? LIMIT 1', ( guid, ))\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        message_id = row[0]\n",
    "        guids[guid] = message_id\n",
    "    except:\n",
    "        print('Could not retrieve guid id',guid)\n",
    "        break\n",
    "\n",
    "cur.close()\n",
    "cur_1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Email data visualization <a name='emviz'></a>\n",
    "Contnuation from the [email data modeling](#email) for data processing and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gbasic.py  <a name='gbasic'></a>\n",
    "Counting and printing out top 10 people/organization of sending emails:  \n",
    "Read in small tables like sender list, subject id list, (without read in the header/body blobs) to dictionaries to save process time. __Caution__ shall be taken on available __memory space__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import zlib\n",
    "\n",
    "howmany = int(input(\"How many to dump? \"))\n",
    "\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT id, sender FROM Senders')\n",
    "senders = dict()\n",
    "for message_row in cur :\n",
    "    senders[message_row[0]] = message_row[1]\n",
    "\n",
    "cur.execute('SELECT id, subject FROM Subjects')\n",
    "subjects = dict()\n",
    "for message_row in cur :\n",
    "    subjects[message_row[0]] = message_row[1]\n",
    "\n",
    "# cur.execute('SELECT id, guid,sender_id,subject_id,headers,body FROM Messages')\n",
    "cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "messages = dict()\n",
    "for message_row in cur :\n",
    "    messages[message_row[0]] = (message_row[1],message_row[2],message_row[3],message_row[4])\n",
    "\n",
    "print(\"Loaded messages=\",len(messages),\"subjects=\",len(subjects),\"senders=\",len(senders))\n",
    "\n",
    "sendcounts = dict()\n",
    "sendorgs = dict()\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    sendcounts[sender] = sendcounts.get(sender,0) + 1\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    sendorgs[dns] = sendorgs.get(dns,0) + 1\n",
    "\n",
    "print('')\n",
    "print('Top',howmany,'Email list participants')\n",
    "\n",
    "x = sorted(sendcounts, key=sendcounts.get, reverse=True)\n",
    "for k in x[:howmany]:\n",
    "    print(senders[k], sendcounts[k])\n",
    "    if sendcounts[k] < 10 : break\n",
    "\n",
    "print('')\n",
    "print('Top',howmany,'Email list organizations')\n",
    "\n",
    "x = sorted(sendorgs, key=sendorgs.get, reverse=True)\n",
    "for k in x[:howmany]:\n",
    "    print(k, sendorgs[k])\n",
    "    if sendorgs[k] < 10 : break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gline.py  <a name='gline'></a>\n",
    "Like `Gbasic.py` grab the top 10 email sending organizations, then break the data by months and visualiza with a line (trend) graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import zlib\n",
    "\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT id, sender FROM Senders')\n",
    "senders = dict()\n",
    "for message_row in cur :\n",
    "    senders[message_row[0]] = message_row[1]\n",
    "\n",
    "cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "messages = dict()\n",
    "for message_row in cur :\n",
    "    messages[message_row[0]] = (message_row[1],message_row[2],message_row[3],message_row[4])\n",
    "\n",
    "print(\"Loaded messages=\",len(messages),\"senders=\",len(senders))\n",
    "\n",
    "sendorgs = dict()\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    sendorgs[dns] = sendorgs.get(dns,0) + 1\n",
    "\n",
    "# pick the top schools\n",
    "orgs = sorted(sendorgs, key=sendorgs.get, reverse=True)\n",
    "orgs = orgs[:10]\n",
    "print(\"Top 10 Organizations\")\n",
    "print(orgs)\n",
    "\n",
    "counts = dict()\n",
    "months = list()\n",
    "# cur.execute('SELECT id, guid,sender_id,subject_id,sent_at FROM Messages')\n",
    "for (message_id, message) in list(messages.items()):\n",
    "    sender = message[1]\n",
    "    pieces = senders[sender].split(\"@\")\n",
    "    if len(pieces) != 2 : continue\n",
    "    dns = pieces[1]\n",
    "    if dns not in orgs : continue\n",
    "    month = message[3][:7]\n",
    "    if month not in months : months.append(month)\n",
    "    key = (month, dns)\n",
    "    counts[key] = counts.get(key,0) + 1\n",
    "\n",
    "months.sort()\n",
    "# print counts\n",
    "# print months\n",
    "\n",
    "fhand = open('gline.js','w')\n",
    "fhand.write(\"gline = [ ['Month'\")\n",
    "for org in orgs:\n",
    "    fhand.write(\",'\"+org+\"'\")\n",
    "fhand.write(\"]\")\n",
    "\n",
    "for month in months:\n",
    "    fhand.write(\",\\n['\"+month+\"'\")\n",
    "    for org in orgs:\n",
    "        key = (month, org)\n",
    "        val = counts.get(key,0)\n",
    "        fhand.write(\",\"+str(val))\n",
    "    fhand.write(\"]\");\n",
    "\n",
    "fhand.write(\"\\n];\\n\")\n",
    "fhand.close()\n",
    "\n",
    "print(\"Output written to gline.js\")\n",
    "print(\"Open gline.htm to visualize the data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gword.py  <a name='gword'></a>\n",
    "Counting all email subjects for word frequencies (numbers and punctuations are being ignored) and use word cloud to present the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import zlib\n",
    "import string\n",
    "\n",
    "conn = sqlite3.connect('index.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT id, subject FROM Subjects')\n",
    "subjects = dict()\n",
    "for message_row in cur :\n",
    "    subjects[message_row[0]] = message_row[1]\n",
    "\n",
    "# cur.execute('SELECT id, guid,sender_id,subject_id,headers,body FROM Messages')\n",
    "cur.execute('SELECT subject_id FROM Messages')\n",
    "counts = dict()\n",
    "for message_row in cur :\n",
    "    text = subjects[message_row[0]]\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    text = text.translate(str.maketrans('','','1234567890'))\n",
    "    # translate all unwated chars to ','\n",
    "    # this is because translate is a C like function\n",
    "    # that directly changes memory stored values thus very fast\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if len(word) < 4 : continue\n",
    "        counts[word] = counts.get(word,0) + 1\n",
    "\n",
    "x = sorted(counts, key=counts.get, reverse=True)\n",
    "highest = None\n",
    "lowest = None\n",
    "for k in x[:100]:\n",
    "    if highest is None or highest < counts[k] :\n",
    "        highest = counts[k]\n",
    "    if lowest is None or lowest > counts[k] :\n",
    "        lowest = counts[k]\n",
    "print('Range of counts:',highest,lowest)\n",
    "\n",
    "# Spread the font sizes across 20-100 based on the count\n",
    "bigsize = 80\n",
    "smallsize = 20\n",
    "# Normalizing/adjusting the font size\n",
    "fhand = open('gword.js','w')\n",
    "fhand.write(\"gword = [\")\n",
    "first = True\n",
    "for k in x[:100]:\n",
    "    if not first : fhand.write( \",\\n\")\n",
    "    first = False\n",
    "    size = counts[k]\n",
    "    size = (size - lowest) / float(highest - lowest)\n",
    "    size = int((size * bigsize) + smallsize)\n",
    "    fhand.write(\"{text: '\"+k+\"', size: \"+str(size)+\"}\")\n",
    "fhand.write( \"\\n];\\n\")\n",
    "fhand.close()\n",
    "\n",
    "print(\"Output written to gword.js\")\n",
    "print(\"Open gword.htm in a browser to see the vizualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .  <a name=''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .  <a name=''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .  <a name=''></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
