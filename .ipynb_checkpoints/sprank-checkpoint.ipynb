{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting existing crawl. Remove the sqlite file to start a new crawl.\n",
      "['https://serenesforest.net/wiki', 'https://serenesforest.net/three-houses', 'http://www.dr-chuck.com', 'https://www.bio.purdue.edu']\n",
      "How many pages (0 = quit):1\n",
      "1\n",
      " 86 https://www.bio.purdue.edu/About/partnerships-centers.html :-) (30707) 68 valid link(s).\n",
      "How many pages (0 = quit):0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_url = 'https://www.bio.purdue.edu/'\n",
    "\n",
    "# Security bridge\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    url TEXT UNIQUE,\n",
    "    html TEXT,\n",
    "    error INTEGER,\n",
    "    old_rank REAL,\n",
    "    new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs \n",
    "    (url TEXT UNIQUE)\n",
    "''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links (\n",
    "    from_id INTEGER,\n",
    "    to_id INTEGER,\n",
    "    UNIQUE (from_id, to_id))\n",
    "''')\n",
    "\n",
    "###\n",
    "def url_end_trim(start_url):\n",
    "    web = start_url.strip(' /')\n",
    "    # strip blank and /\n",
    "    if web.endswith('.htm') or web.endswith('.html'):\n",
    "        pos = web.rfind('/')\n",
    "        # find in reverse the last / sign\n",
    "        web = web[:pos]\n",
    "    return web\n",
    "\n",
    "###\n",
    "def enter_number():\n",
    "    while True:\n",
    "        many = input('How many pages (0 = quit):')\n",
    "        # print('in', type(many), many)\n",
    "        try:\n",
    "            many = int(many)\n",
    "            if many >= 0:\n",
    "                break\n",
    "            # only accepting number > 0\n",
    "        except:\n",
    "            pass\n",
    "        print('Invalid input. Please enter an integer >= 1.')\n",
    "    return many\n",
    "\n",
    "###\n",
    "def href_filter(href, url):\n",
    "    up = urlparse(href)\n",
    "    if len(up.scheme) <1:\n",
    "        href = urljoin(url, href)\n",
    "        # if no scheme, join the original url with the new href path (href being a subunit of url)\n",
    "    ipos = href.find('#')\n",
    "    if ipos > 1:\n",
    "        href = href[:ipos]\n",
    "    if href.endswith('/'):\n",
    "        href = href[:-1]\n",
    "    if href.endswith('.png') or href.endswith('.jpg') or href.endswith('.png'):\n",
    "        href = None\n",
    "#     if len(href) < 1:\n",
    "#         href = None\n",
    "    # return None for invalid href\n",
    "    return href\n",
    "\n",
    "###\n",
    "def in_webs_range(href, webs):\n",
    "    # define if the page inside the set of webs we want to limit our search to\n",
    "    # return True if in range, false otherwise\n",
    "    if len(webs) < 1:\n",
    "        print('Error: no web range defined.')\n",
    "        return False\n",
    "    for web in webs:\n",
    "        if href.startswith(web):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Initialize\n",
    "cur.execute('''SELECT id, url FROM Pages WHERE html IS NULL AND error IS NULL ORDER BY RANDOM() LIMIT 1''')\n",
    "row = cur.fetchone()\n",
    "# print(f'{type(row)} :::{row}::')\n",
    "if row is not None:\n",
    "    # if databse is not empty\n",
    "    print('Restarting existing crawl. Remove the sqlite file to start a new crawl.')\n",
    "else:\n",
    "    start_url = start_url.strip()\n",
    "    web = url_end_trim(start_url)\n",
    "    # formating the web url\n",
    "    if len(web) > 1:\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES (?)', (web, ))\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES (?, NULL, 1.0)', (start_url, ))\n",
    "        conn.commit() # write to the hard drive\n",
    " \n",
    "# set up webs list\n",
    "webs = []\n",
    "cur.execute('SELECT url FROM Webs')\n",
    "for row in cur:\n",
    "#     print(row) # each row is a tuple of 1 element (url, )\n",
    "    webs.append(str(row[0]))\n",
    "    \n",
    "print(webs)\n",
    "# printing all the (unfinished) webs in the database\n",
    "\n",
    "# main\n",
    "while True:\n",
    "#     print(f'Loop1:{row}')\n",
    "    many = enter_number()\n",
    "    print(many)\n",
    "    if many < 1:\n",
    "        break\n",
    "    \n",
    "    while many > 0:\n",
    "#         print(f'{many} times left')\n",
    "#         print(f'Loop2a:{row}')\n",
    "        cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "        try:\n",
    "#             print(f'Loop2b:{row}')\n",
    "            row = cur.fetchone()\n",
    "#             print(f'Loop2c:{row}')\n",
    "            fromid = row[0]\n",
    "            url = row[1]\n",
    "        except:\n",
    "            print('No un-retrieved HTML Pages found.')\n",
    "            many = 0\n",
    "            break\n",
    "        print(f'{fromid: 3} {url}', end=' ')\n",
    "        \n",
    "        # now we already have the url and id of the page\n",
    "        cur.execute('DELETE FROM Links WHERE from_id = ?', (fromid,))\n",
    "        try:\n",
    "            doc = urlopen(url, context=ctx)\n",
    "            print(':-)', end=' ')\n",
    "            status = doc.getcode()\n",
    "            # get status code for request\n",
    "            if status != 200:\n",
    "                print('Error on page:', doc.getcode())\n",
    "                cur.execute('UPDATE Pages SET (error=?) WHERE url=?)', (status, url))\n",
    "            if doc.info().get_content_type() != 'text/html':\n",
    "                print('Ignore non text/html page.')\n",
    "                cur.execute('DELETE FROM Pages WHERE url=?', (url,))\n",
    "                conn.commit()\n",
    "                continue \n",
    "            html = doc.read()\n",
    "            print(f'({len(html)})', end=' ')\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nProgram interrupted by user...')\n",
    "            break\n",
    "        except:\n",
    "            print('Unable to retrieve or parse the page')\n",
    "            cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ))\n",
    "            conn.commit()\n",
    "            continue\n",
    "            \n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES (?, NULL, 1.0)', (start_url, )) \n",
    "        # make sure the page is in the db\n",
    "        cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url))\n",
    "        # update the html content of the page\n",
    "        conn.commit()\n",
    "        \n",
    "        # retrieve all of the anchor tags\n",
    "        tags = soup('a')\n",
    "        count = 0\n",
    "        for tag in tags:\n",
    "            href = tag.get('href', None)\n",
    "            href = href_filter(href, url)\n",
    "            # if href is None or not passing the filter\n",
    "            if href is None:\n",
    "                continue\n",
    "            # if href is not in webs range\n",
    "            if in_webs_range(href, webs) is False:\n",
    "                continue\n",
    "                \n",
    "            cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES (?, NULL, 1.0)', (href,))\n",
    "            count += 1\n",
    "            conn.commit()\n",
    "            \n",
    "            # retrieve the to_id\n",
    "            cur.execute('SELECT id FROM Pages WHERE url=?', (href,))\n",
    "            try:\n",
    "                row = cur.fetchone()\n",
    "                toid = row[0]\n",
    "            except:\n",
    "                print('Could not retrieve id.')\n",
    "                continue\n",
    "            # insert into the Links table    \n",
    "            cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES (?, ?)', (fromid, toid))\n",
    "                \n",
    "            \n",
    "        print(f'{count} valid link(s).')    \n",
    "        many -= 1\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
