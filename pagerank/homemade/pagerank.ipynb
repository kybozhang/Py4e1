{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spider\n",
    "A page crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting existing crawl. Remove the sqlite file to start a new crawl.\n",
      "['https://serenesforest.net/wiki', 'https://serenesforest.net/three-houses', 'http://www.dr-chuck.com', 'https://www.bio.purdue.edu']\n",
      "How many pages (0 = quit):6\n",
      "6\n",
      " 2780 https://www.bio.purdue.edu/lab/leung/blog/?feed=rss2&p=1836 :-) Ignore non text/html page.\n",
      " 1618 https://www.bio.purdue.edu/lab/leung/blog/?tag=dmd :-) (25025) 45 valid link(s).\n",
      " 1046 https://www.bio.purdue.edu/resources/illustration/eco/files_images/lucas/2006.Chapter%2013%20-%20Lucas%20&%20%20Freeberg%20final.REAL.pdf :-) Ignore non text/html page.\n",
      " 2334 https://www.bio.purdue.edu/bionews/articles/documents/articles/2019/Mattoo 4 12 19.html Unable to retrieve or parse the page\n",
      " 1497 https://www.bio.purdue.edu/lab/leung/blog/?tag=microscopy :-) (36624) 63 valid link(s).\n",
      " 2301 https://www.bio.purdue.edu/bionews/articles/Academic/People/alumni/articles/2019/deng-research-neutrophil-migration.html :-) (68021) 94 valid link(s).\n",
      " 1486 https://www.bio.purdue.edu/lab/leung/blog/?p=1729 :-) (28387) 49 valid link(s).\n",
      " 2602 https://www.bio.purdue.edu/bionews/articles/People/alumni/articles/People/Academic/People/People/faculty/positions.html :-) (68021) 94 valid link(s).\n",
      " 1177 https://www.bio.purdue.edu/lab/leung/blog/?attachment_id=2428 :-) (26542) 42 valid link(s).\n",
      "Job complete!\n",
      "How many pages (0 = quit):1\n",
      "1\n",
      " 1269 https://www.bio.purdue.edu/lab/leung/blog/?tag=retina :-) (101574) 198 valid link(s).\n",
      "Job complete!\n",
      "How many pages (0 = quit):0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "start_url = 'https://www.bio.purdue.edu/'\n",
    "\n",
    "# Security bridge\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    url TEXT UNIQUE,\n",
    "    html TEXT,\n",
    "    error INTEGER,\n",
    "    old_rank REAL,\n",
    "    new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs \n",
    "    (url TEXT UNIQUE)\n",
    "''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links (\n",
    "    from_id INTEGER,\n",
    "    to_id INTEGER,\n",
    "    UNIQUE (from_id, to_id))\n",
    "''')\n",
    "\n",
    "###\n",
    "def url_end_trim(start_url):\n",
    "    web = start_url.strip(' /')\n",
    "    # strip blank and /\n",
    "    if web.endswith('.htm') or web.endswith('.html'):\n",
    "        pos = web.rfind('/')\n",
    "        # find in reverse the last / sign\n",
    "        web = web[:pos]\n",
    "    return web\n",
    "\n",
    "###\n",
    "def enter_number():\n",
    "    while True:\n",
    "        many = input('How many pages (0 = quit):')\n",
    "        # print('in', type(many), many)\n",
    "        try:\n",
    "            many = int(many)\n",
    "            if many >= 0:\n",
    "                break\n",
    "            # only accepting number > 0\n",
    "        except:\n",
    "            pass\n",
    "        print('Invalid input. Please enter an integer >= 1.')\n",
    "    return many\n",
    "\n",
    "###\n",
    "def href_filter(href, url):\n",
    "    up = urlparse(href)\n",
    "    if len(up.scheme) <1:\n",
    "        href = urljoin(url, href)\n",
    "        # if no scheme, join the original url with the new href path (href being a subunit of url)\n",
    "    ipos = href.find('#')\n",
    "    if ipos > 1:\n",
    "        href = href[:ipos]\n",
    "    if href.endswith('/'):\n",
    "        href = href[:-1]\n",
    "    if href.endswith('.png') or href.endswith('.jpg') or href.endswith('.png'):\n",
    "        href = None\n",
    "#     if len(href) < 1:\n",
    "#         href = None\n",
    "    # return None for invalid href\n",
    "    return href\n",
    "\n",
    "###\n",
    "def in_webs_range(href, webs):\n",
    "    # define if the page inside the set of webs we want to limit our search to\n",
    "    # return True if in range, false otherwise\n",
    "    if len(webs) < 1:\n",
    "        print('Error: no web range defined.')\n",
    "        return False\n",
    "    for web in webs:\n",
    "        if href.startswith(web):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Initialize\n",
    "cur.execute('''SELECT id, url FROM Pages WHERE html IS NULL AND error IS NULL ORDER BY RANDOM() LIMIT 1''')\n",
    "row = cur.fetchone()\n",
    "# print(f'{type(row)} :::{row}::')\n",
    "if row is not None:\n",
    "    # if databse is not empty\n",
    "    print('Restarting existing crawl. Remove the sqlite file to start a new crawl.')\n",
    "else:\n",
    "    start_url = start_url.strip()\n",
    "    web = url_end_trim(start_url)\n",
    "    # formating the web url\n",
    "    if len(web) > 1:\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES (?)', (web, ))\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES (?, NULL, 1.0)', (start_url, ))\n",
    "        conn.commit() # write to the hard drive\n",
    " \n",
    "# set up webs list\n",
    "webs = []\n",
    "cur.execute('SELECT url FROM Webs')\n",
    "for row in cur:\n",
    "#     print(row) # each row is a tuple of 1 element (url, )\n",
    "    webs.append(str(row[0]))\n",
    "    \n",
    "print(webs)\n",
    "# printing all the (unfinished) webs in the database\n",
    "\n",
    "# main\n",
    "while True:\n",
    "#     print(f'Loop1:{row}')\n",
    "    many = enter_number()\n",
    "    print(many)\n",
    "    if many < 1:\n",
    "        break\n",
    "    \n",
    "    while many > 0:\n",
    "#         print(f'{many} times left')\n",
    "#         print(f'Loop2a:{row}')\n",
    "        cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "        try:\n",
    "#             print(f'Loop2b:{row}')\n",
    "            row = cur.fetchone()\n",
    "#             print(f'Loop2c:{row}')\n",
    "            fromid = row[0]\n",
    "            url = row[1]\n",
    "        except:\n",
    "            print('No un-retrieved HTML Pages found.')\n",
    "            many = 0\n",
    "            break\n",
    "        print(f'{fromid: 3} {url}', end=' ')\n",
    "        \n",
    "        # now we already have the url and id of the page\n",
    "        cur.execute('DELETE FROM Links WHERE from_id = ?', (fromid,))\n",
    "        try:\n",
    "            doc = urlopen(url, context=ctx)\n",
    "            print(':-)', end=' ')\n",
    "            status = doc.getcode()\n",
    "            # get status code for request\n",
    "            if status != 200:\n",
    "                print('Error on page:', doc.getcode())\n",
    "                cur.execute('UPDATE Pages SET (error=?) WHERE url=?)', (status, url))\n",
    "            if doc.info().get_content_type() != 'text/html':\n",
    "                print('Ignore non text/html page.')\n",
    "                cur.execute('DELETE FROM Pages WHERE url=?', (url,))\n",
    "                conn.commit()\n",
    "                continue \n",
    "            html = doc.read()\n",
    "            print(f'({len(html)})', end=' ')\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nProgram interrupted by user...')\n",
    "            break\n",
    "        except:\n",
    "            print('Unable to retrieve or parse the page')\n",
    "            cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ))\n",
    "            conn.commit()\n",
    "            continue\n",
    "            \n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES (?, NULL, 1.0)', (start_url, )) \n",
    "        # make sure the page is in the db\n",
    "        cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url))\n",
    "        # update the html content of the page\n",
    "        conn.commit()\n",
    "        \n",
    "        # retrieve all of the anchor tags\n",
    "        tags = soup('a')\n",
    "        count = 0\n",
    "        for tag in tags:\n",
    "            href = tag.get('href', None)\n",
    "            href = href_filter(href, url)\n",
    "            # if href is None or not passing the filter\n",
    "            if href is None:\n",
    "                continue\n",
    "            # if href is not in webs range\n",
    "            if in_webs_range(href, webs) is False:\n",
    "                continue\n",
    "                \n",
    "            cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES (?, NULL, 1.0)', (href,))\n",
    "            count += 1\n",
    "            conn.commit()\n",
    "            \n",
    "            # retrieve the to_id\n",
    "            cur.execute('SELECT id FROM Pages WHERE url=?', (href,))\n",
    "            try:\n",
    "                row = cur.fetchone()\n",
    "                toid = row[0]\n",
    "            except:\n",
    "                print('Could not retrieve id.')\n",
    "                continue\n",
    "            # insert into the Links table    \n",
    "            cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES (?, ?)', (fromid, toid))\n",
    "                \n",
    "            \n",
    "        print(f'{count} valid link(s).')    \n",
    "        many -= 1\n",
    "        if many < 1:\n",
    "            print('Job complete!')\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprank\n",
    "A ranking algorithm\n",
    ">Basic idea: \n",
    "- loop(new rank = old rank + sum(inbound link contributions)); \n",
    "- Inbound link contribution = inbound page rank / links on inbound page.\n",
    "- Evaporation corrsction = sum(abs(old rank - new rank)) / no of pages\n",
    "\n",
    "Since my database (from bio.purdue) contains many one-sided links, the evediff doesn't seem to decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many pages (0 = quit):1\n",
      "1 - 1.2097475291280309\n",
      "[(3, 0.00107752559374563), (4, 0.3030108524614837), (6, 0.10824243064463338), (7, 0.10824243064463338), (8, 0.27043564965607847), (13, 0.13204101370982219)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "###\n",
    "def enter_number():\n",
    "    while True:\n",
    "        many = input('How many pages (0 = quit):')\n",
    "        # print('in', type(many), many)\n",
    "        try:\n",
    "            many = int(many)\n",
    "            if many >= 0:\n",
    "                break\n",
    "            # only accepting number > 0\n",
    "        except:\n",
    "            pass\n",
    "        print('Invalid input. Please enter an integer >= 1.')\n",
    "    return many\n",
    "\n",
    "# Find the ids that send out page rank - we only are interested\n",
    "# in pages in the SCC that have in and out links\n",
    "cur.execute('SELECT DISTINCT from_id FROM Links')\n",
    "fids = []\n",
    "for row in cur:\n",
    "    fids.append(row[0])\n",
    "\n",
    "# Find the ids that receive page rank\n",
    "cur.execute('SELECT DISTINCT from_id, to_id FROM Links')\n",
    "tids = []\n",
    "links = []\n",
    "# only look for: !) valid from_id 2) not self linking 3) also being linked\n",
    "for row in cur:\n",
    "    if row[0] in fids:\n",
    "        if row[0] != row[1]:\n",
    "            if row[1] in fids:\n",
    "                links.append(row)\n",
    "                if row[1] not in tids:\n",
    "                    tids.append(row[1])\n",
    "# check the result\n",
    "# print(tids)\n",
    "\n",
    "# Get latest page ranks for strongly connected component\n",
    "oranks= dict()\n",
    "for fid in fids:\n",
    "    cur.execute('SELECT new_rank FROM Pages WHERE id=?', (fid,))\n",
    "    row = cur.fetchone()\n",
    "    oranks[fid] = row[0]\n",
    " \n",
    "many = enter_number()\n",
    "if many == 0:\n",
    "    quit()\n",
    "\n",
    "# Lets do Page Rank in memory so it is really fast\n",
    "for i in range(many):\n",
    "    # print prev_ranks.items()[:5]\n",
    "    nranks= dict()\n",
    "    total = 0.0\n",
    "    for (fid, rank) in list(oranks.items()):\n",
    "        total += rank\n",
    "        nranks[fid] = 0.0\n",
    "#     print('Old total:', total)\n",
    "    \n",
    "    # Find the number of outbound links and sent the page rank down each\n",
    "    for (fid, rank) in list(oranks.items()):\n",
    "        outlinks = []\n",
    "        for (from_id, to_id) in links:\n",
    "            if from_id == fid:\n",
    "                if to_id in tids:\n",
    "                    outlinks.append(to_id)\n",
    "        n = len(outlinks) # no of out links\n",
    "#         print(f'{fid}:: {n} outbound links.')\n",
    "        if n > 0:\n",
    "            diff = rank / n\n",
    "            for tid in outlinks:\n",
    "                nranks[tid] = nranks[tid] + diff\n",
    "    \n",
    "    # calculate new total\n",
    "    newtot = 0.0\n",
    "    for (fid, rank) in list(nranks.items()):\n",
    "        newtot += rank\n",
    "    \n",
    "    # compute the evaporation value => keep the total unchanged\n",
    "    evap = (total - newtot)/ len(nranks) \n",
    "    for fid in nranks:\n",
    "        nranks[fid] = nranks[fid] + evap\n",
    "        # correct each term by evap to avoid local trapping\n",
    "        \n",
    "#     # calculate new total\n",
    "#     newtot = 0.0\n",
    "#     for (fid, rank) in list(nranks.items()):\n",
    "#         newtot += rank\n",
    "#     print('New total:', total)\n",
    "        \n",
    "    # Compute the per-page average change from old rank to new rank\n",
    "    # As indication of convergence of the algorithm\n",
    "    totaldiff = 0\n",
    "    for (fid, old) in list(oranks.items()):\n",
    "        diff = abs(nranks[tid] - old)\n",
    "        totaldiff += diff\n",
    "    avediff = totaldiff / len(nranks)\n",
    "    print(f'{i+1} - {avediff}')\n",
    "    \n",
    "    # ratate values\n",
    "    oranks = nranks\n",
    "\n",
    "# Put the final ranks back into the database\n",
    "print(list(nranks.items())[:6]) \n",
    "cur.execute('UPDATE Pages SET old_rank=new_rank')\n",
    "for (fid, rank) in list(nranks.items()):\n",
    "    cur.execute('UPDATE Pages SET new_rank=? WHERE id=?', (rank, fid))\n",
    "conn.commit()\n",
    "# close\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pages set to a rank of 1.0\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''UPDATE Pages SET new_rank=1.0, old_rank=0.0''')\n",
    "conn.commit()\n",
    "\n",
    "cur.close()\n",
    "\n",
    "print(\"All pages set to a rank of 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spdump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 10.894714562863069, 10.861087065802701, 42, 'https://www.bio.purdue.edu')\n",
      "(411, 10.35736889253767, 10.374112810411978, 43, 'https://www.bio.purdue.edu/index.php')\n",
      "(411, 10.321220329015757, 10.337896496679763, 80, 'https://www.bio.purdue.edu/life-sciences-postdoc/index.html')\n",
      "(410, 10.289932338348184, 10.306544006338136, 44, 'https://www.bio.purdue.edu/About/index.php')\n",
      "(410, 10.289932338348184, 10.306544006338136, 46, 'https://www.bio.purdue.edu/About/facts.html')\n",
      "(410, 10.289932338348184, 10.306544006338136, 47, 'https://www.bio.purdue.edu/About/contact.html')\n",
      "(410, 10.289932338348184, 10.306544006338136, 48, 'https://www.bio.purdue.edu/About/diversity.html')\n",
      "(410, 10.289932338348184, 10.306544006338136, 66, 'https://www.bio.purdue.edu/About/biology-research-areas.html')\n",
      "(410, 10.400073256702127, 10.416898596257333, 67, 'https://www.bio.purdue.edu/bio-education/index.html')\n",
      "(410, 10.406818820086418, 10.423656813182879, 68, 'https://www.bio.purdue.edu/cell/index.html')\n",
      "(410, 10.413107056156809, 10.429956845120891, 69, 'https://www.bio.purdue.edu/ecology/index.html')\n",
      "(410, 10.424485766749262, 10.441356901071691, 70, 'https://www.bio.purdue.edu/microbiology/index.html')\n",
      "(410, 10.416093967968331, 10.432949360033172, 71, 'https://www.bio.purdue.edu/neuroscience/index.html')\n",
      "(410, 10.314081346191372, 10.330743430670701, 73, 'https://www.bio.purdue.edu/Outreach/index.html')\n",
      "(410, 10.320519912742117, 10.337194057113464, 74, 'https://www.bio.purdue.edu/Outreach/biologyoutreach.html')\n",
      "(410, 10.344280556220475, 10.360991873242638, 77, 'https://www.bio.purdue.edu/Resources/index.html')\n",
      "(410, 10.283542744497716, 10.300140320564516, 79, 'https://www.bio.purdue.edu/Resources/documents.html')\n",
      "(409, 10.31053882369155, 10.32719907563968, 49, 'https://www.bio.purdue.edu/Academic/undergrad/prospective.html')\n",
      "(409, 10.325815896597065, 10.342504789661994, 50, 'https://www.bio.purdue.edu/Academic/undergrad/index.html')\n",
      "(409, 10.304972734677294, 10.321622554600877, 51, 'https://www.bio.purdue.edu/Academic/graduate/index.html')\n",
      "630 rows.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "     FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "     WHERE html IS NOT NULL\n",
    "     GROUP BY id ORDER BY inbound DESC''')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    if count < 20 : print(row)\n",
    "    count = count + 1\n",
    "print(count, 'rows.')\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating JSON output on spider.js...\n",
      "How many nodes? 39\n",
      "Open force.html in a browser to view the visualization\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Creating JSON output on spider.js...\")\n",
    "howmany = int(input(\"How many nodes? \"))\n",
    "\n",
    "cur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n",
    "    FROM Pages JOIN Links ON Pages.id = Links.to_id\n",
    "    WHERE html IS NOT NULL AND ERROR IS NULL\n",
    "    GROUP BY id ORDER BY id,inbound''')\n",
    "\n",
    "fhand = open('spider.js','w')\n",
    "nodes = list()\n",
    "maxrank = None\n",
    "minrank = None\n",
    "for row in cur :\n",
    "    nodes.append(row)\n",
    "    rank = row[2]\n",
    "    if maxrank is None or maxrank < rank: maxrank = rank\n",
    "    if minrank is None or minrank > rank : minrank = rank\n",
    "    if len(nodes) > howmany : break\n",
    "\n",
    "if maxrank == minrank or maxrank is None or minrank is None:\n",
    "    print(\"Error - please run sprank.py to compute page rank\")\n",
    "    quit()\n",
    "\n",
    "fhand.write('spiderJson = {\"nodes\":[\\n')\n",
    "count = 0\n",
    "map = dict()\n",
    "ranks = dict()\n",
    "for row in nodes :\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    # print row\n",
    "    rank = row[2]\n",
    "    rank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    # relative rank\n",
    "    fhand.write('{'+'\"weight\":'+str(row[0])+',\"rank\":'+str(rank)+',')\n",
    "    fhand.write(' \"id\":'+str(row[3])+', \"url\":\"'+row[4]+'\"}')\n",
    "    # row[0] is no of inbound nodes\n",
    "    map[row[3]] = count # map the processing order\n",
    "    ranks[row[3]] = rank # rank data\n",
    "    count = count + 1\n",
    "fhand.write('],\\n')\n",
    "\n",
    "cur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\n",
    "fhand.write('\"links\":[\\n')\n",
    "\n",
    "count = 0\n",
    "for row in cur :\n",
    "    # print row\n",
    "    if row[0] not in map or row[1] not in map : continue\n",
    "    if count > 0 : fhand.write(',\\n')\n",
    "    rank = ranks[row[0]]\n",
    "    srank = 19 * ( (rank - minrank) / (maxrank - minrank) ) \n",
    "    fhand.write('{\"source\":'+str(map[row[0]])+',\"target\":'+str(map[row[1]])+',\"value\":3}')\n",
    "    count = count + 1\n",
    "fhand.write(']};')\n",
    "fhand.close()\n",
    "cur.close()\n",
    "\n",
    "print(\"Open force.html in a browser to view the visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
